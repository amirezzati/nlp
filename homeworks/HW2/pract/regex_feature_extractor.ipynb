{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aWhtsBGygww-",
        "PTiwdMlap4U7",
        "617a2-66JzhQ",
        "WcnXDr4VZPu7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Packages & Libraries**"
      ],
      "metadata": {
        "id": "KG1p5anyeXYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hazm"
      ],
      "metadata": {
        "id": "aWhtsBGygww-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1XVzFfmVinB",
        "outputId": "9f76caff-6209-440e-e91a-b6b36bee0be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.10)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install hazm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import & Download hazm models"
      ],
      "metadata": {
        "id": "PTiwdMlap4U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "import hazm\n",
        "import re\n",
        "import pandas as ps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j5WDLVT2hJs",
        "outputId": "9f5ba669-826c-4947-a6e8-571715512ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1rr7j4FHIumIFmqgVsQHbhU8Q0vv7t1TR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayMYbfq1e3_I",
        "outputId": "0c8505b4-f3e8-4329-ca45-7e27b3e1c42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rr7j4FHIumIFmqgVsQHbhU8Q0vv7t1TR\n",
            "To: /content/pos_tagger.model\n",
            "100% 19.2M/19.2M [00:00<00:00, 47.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download_folder('https://drive.google.com/drive/folders/1Zjg-rYwSrk4pXwB1Lm3Q6IfHvyxeJspX?usp=drive_link', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Na6p6-CZeHE",
        "outputId": "0616a8cf-a385-4b8a-b0a7-0f3b874ed763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/universal_dependency_parser/.DS_Store',\n",
              " '/content/universal_dependency_parser/langModel.mco',\n",
              " '/content/universal_dependency_parser/lib/liblinear-1.8.jar',\n",
              " '/content/universal_dependency_parser/lib/libsvm.jar',\n",
              " '/content/universal_dependency_parser/lib/log4j.jar',\n",
              " '/content/universal_dependency_parser/malt-features.xml',\n",
              " '/content/universal_dependency_parser/malt-options.xml',\n",
              " '/content/universal_dependency_parser/malt.jar',\n",
              " '/content/universal_dependency_parser/MaltEval.jar']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop words"
      ],
      "metadata": {
        "id": "617a2-66JzhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O stopwords.txt https://raw.githubusercontent.com/kharazi/persian-stopwords/master/short"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSPUA9tFpVpG",
        "outputId": "32db1a6b-0db2-4e90-f7a7-6806d74e518c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-19 18:51:13--  https://raw.githubusercontent.com/kharazi/persian-stopwords/master/short\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2945 (2.9K) [text/plain]\n",
            "Saving to: â€˜stopwords.txtâ€™\n",
            "\n",
            "stopwords.txt       100%[===================>]   2.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-19 18:51:13 (29.9 MB/s) - â€˜stopwords.txtâ€™ saved [2945/2945]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/augmented_data.txt\n",
        "# !wget https://raw.githubusercontent.com/language-ml/parsi.io/main/parsi_io/modules/product_feature_extractor/data/quality_pre_words.txt\n",
        "# !wget https://github.com/language-ml/parsi.io/blob/main/parsi_io/modules/product_feature_extractor/data/augmented_data.txt\n",
        "!git clone --depth=1 https://github.com/language-ml/parsi.io.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhtfIOTO3hmB",
        "outputId": "6e97edc0-8609-4650-999e-5d1fc85357b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'parsi.io'...\n",
            "remote: Enumerating objects: 245, done.\u001b[K\n",
            "remote: Counting objects: 100% (245/245), done.\u001b[K\n",
            "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
            "remote: Total 245 (delta 14), reused 215 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (245/245), 42.12 MiB | 15.22 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "Updating files: 100% (217/217), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence tokenizer**"
      ],
      "metadata": {
        "id": "WcnXDr4VZPu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments = [\n",
        "    'Ø¨Ø¯ Ù†ÛŒØ³Øª Ú©Ø§Ø± Ø±Ø§Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ ',\n",
        "    'Ø¨Ø§ Ø³Ù„Ø§Ù… Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¨ÛŒ Ø¨ÙˆØ¯ Ø¨Ù‡ Ø¬Ø§',\n",
        "    'Ø·Ø¨Ù‚ Ø§Ø¯Ø¹Ø§ÛŒ Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§ Ù†Ø§Øª Ø§Ú©ØªÛŒÙˆ Ø¨ÙˆØ¯ Ùˆ Ø§Ø² Ù„Ø­Ø§Ø¸ Ù‚ÛŒÙ…ØªÛŒ ØªÙˆ Ø´Ú¯ÙØª Ø§Ù†Ú¯ÛŒØ² Ø®ÙˆØ¨ Ø¨ÙˆØ¯',\n",
        "    'ØªØ§Ø²Ù‡ Ú†Ù†Ø¯ Ø±ÙˆØ²Ù‡ Ø¨Ø¯Ø³ØªÙ… Ø±Ø³ÛŒØ¯Ù‡ ØªØ§ Ø§Ù„Ø§Ù† Ú©Ù‡ Ø®ÙˆØ¨ Ø¨ÙˆØ¯Ù‡ Ø±Ø§Ø¶ÛŒÙ…',\n",
        "    'Ú¯ÙˆØ´ÛŒ Ø¢ÛŒÙÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ¹Ø±ÛŒÙ Ù†Ø¯Ø§Ø±Ù‡ ØŒ Ù…Ù†Ú©Ù‡ Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù… Ø¨ÛŒØ´ØªØ± Ø¯Ù†Ø¨Ø§Ù„ zaa Ø¨ÙˆØ¯Ù… Ø§Ù…Ø§ Ø¯ÛŒ Ø¬ÛŒ Ú©Ø§Ù„Ø§ ch Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª Ù†Ø­ÙˆÙ‡ Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø¹Ø§Ù„ÛŒ',\n",
        "    'Ø§ÛŒÙÙˆÙ† Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ¹Ø±ÛŒÙ Ù†Ø¯Ø§Ø±Ù‡ Ø¹Ø§Ù„ÛŒÙ‡',\n",
        "    'Ø®ÙˆØ¨Ù‡ Ù…Ù† Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù… Ù‡Ù…Ù‡ Ú†ÛŒØ² Ù…Ø±ØªØ¨ Ùˆ Ø¨Ø¯ÙˆÙ† Ø§ÛŒØ±Ø§Ø¯',\n",
        "    'Ù‚ÛŒÙ…Øª Ù…Ù†Ø§Ø³Ø¨ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¨Ø§Ø²Ø§Ø± Ùˆ Ù†Ø§Øª Ø§Ú©ØªÛŒÙˆ',\n",
        "    'Ø®ÙˆØ¨ ÙˆØ¹Ø§Ù„ÛŒ ÙˆÙ…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯Ù…Ù…Ù†ÙˆÙ†Ù…',\n",
        "    'Ø®ÙˆØ¨Ù‡ Ø¯Ø± Ú©Ù„ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ù†Ø¯Ù‡ Ø§Ø³ ',\n",
        "    'ÙˆØ§Ù‚Ø¹Ø§ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ù…Ù…Ù†ÙˆÙ†Ù… Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§',\n",
        "    'Ù‡Ù…Ù‡ Ú†ÛŒ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯',\n",
        "    'Ù…Ù† Ø³ÙØ§Ø±Ø´ Ø¯Ø§Ø¯Ù… ÙˆÙ‚ØªÛŒ Ø¨Ø³ØªÙ‡ Ø±Ø³ÛŒØ¯ Ø¯ÛŒØ¯Ù… Ù¾Ù„Ù…Ù¾Ø´ Ø¨Ø§Ø² Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ Ú©Ù‡ Ø§ØµÙ„Ø§ Ú©Ø§Ø± Ù‚Ø´Ù†Ú¯ÛŒ Ù†ÛŒØ³Øª Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§ Ùˆ ÙØ±ÙˆØ´Ù†Ø¯Ù‡',\n",
        "    'Ø¨Ø§Ø² Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ ØªÙ…ÛŒØ² Ùˆ Ø³Ø§Ù„Ù… Ø¨Ø¯Ø³ØªÙ… Ø±Ø³ÛŒØ¯',\n",
        "    'Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ø³Ø§Ù„Ù… Ùˆ Ù¾Ù„Ù…Ù¾ Ø¨Ù‡ Ø¯Ø³ØªÙ… Ø±Ø³ÛŒØ±',\n",
        "    'Ú¯ÙˆØ´ÛŒ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ÛŒ Ù‡Ø³Øª ÙÙ‚Ø· Ø¨Ø§ÛŒØ¯ Ø­ÙˆØ§Ø³ØªÙˆÙ† Ø¨Ø§Ø´Ù‡ Ø§Ù¾Ù„ Ø¢ÛŒØ¯ÛŒ Ø±Ùˆ Ú¯Ù… Ù†Ú©Ù†ÛŒØ¯ Ø±Ù†Ú¯ Ø³Ø¨Ø²Ø´ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´ Ø±Ù†Ú¯ ØªØ± Ø§Ø² Ø¨Ù‚ÛŒÙ‡ Ø±Ù†Ú¯ Ù‡Ø§ Ù‡Ø³Øª Ù„ÙˆØ§Ø²Ù… Ø¬Ø§Ù†Ø¨ÛŒ Ù‡Ø±Ú†ÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ† Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ú†ÙˆÙ† Ø¨ÛŒØ±ÙˆÙ† Ù‚ÛŒÙ…Øª Ù‡Ø§ Ø®ÛŒÙ„ÛŒ Ø¨Ø§Ù„Ø§ÛŒÙ‡',\n",
        "    'Ø¹Ø§Ù„ÛŒ Ù†Ø§Øª Ø§Ú©ÛŒØªÙˆ Ù‡Ù…ÙˆÙ† Ø±ÙˆØ² Ø®Ø±ÛŒØ¯ ØªØ­ÙˆÛŒÙ„ Ø´Ø¯',\n",
        "    'Ù…Ù† ØªÙˆÛŒ Ø´Ú¯ÙØªØ§Ù†Ù‡ Ø®Ø±ÛŒØ¯Ù… ØªÙ‚Ø±ÛŒØ¨Ø§ Ø¯Ùˆ ØªÙˆÙ…Ù† Ø²ÛŒØ± Ù‚ÛŒÙ…Øª Ø¨Ø§Ø²Ø§Ø± Ú¯Ø±ÙØªÙ… Ø±ÛŒØ¬Ø³ØªØ±ÛŒ Ù‡Ù… Ø´Ø¯ Ù…Ø´Ú©Ù„ÛŒ Ù†Ø¯Ø§Ø´Øª',\n",
        "    'Ø¨Ø³ÛŒØ§Ø± Ø¹Ø§Ù„ÛŒ Ùˆ Ø¨Ø§Ú©ÛŒÙÛŒØª Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ ØªØ¹Ø±ÛŒÙ Ù†ÛŒØ³Øª',\n",
        "    'Û´_ÛµØ³Ø§Ø¹ØªÙ‡ Ø¨Ø±Ø§Ù… Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ Ùˆ Ú¯ÙˆØ´ÛŒ Ù†Ø§Øª Ø§Ú©ØªÛŒÙˆ Ø¨ÙˆØ¯Ù† Ùˆ Ø±ÛŒØ¬Ø³ØªØ± Ø´Ø¯',\n",
        "    'Ø¯Ùˆ Ù‡ÙØªÙ‡ Ø¯ÛŒØ±ØªØ± ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù†Ø¯.Ø±ÛŒØ¬Ø³ØªØ±ÛŒØ´ Ù…Ø´Ú©Ù„ Ø¯Ø§Ø´ØªØŒÚ©Ù„ÛŒ ÙˆÙ‚ØªÙ… Ø¨Ø±Ø§ÛŒ ÛŒÙ‡ Ú¯ÙˆØ´ÛŒ ØªÙ„Ù Ø´Ø¯',\n",
        "    'Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ùˆ Ø¨Ø± Ø®Ù„Ø§Ù ÛŒØ³Ø±ÛŒ Ø§Ø² Ù†Ø¸Ø±Ø§Øª Ú¯ÙˆØ´ÛŒ Ø§ØµÙ„ Ùˆ Ù¾Ù„Ù…Ù¾ Ùˆ Ù†Ø§Øª Ø§Ú©ØªÛŒÙˆ Ø¨ÙˆØ¯ Ùˆ Ø¨Ù‡ Ø±Ø§Ø­ØªÛŒ Ø±ÛŒØ¬Ø³ØªØ± Ø´Ø¯ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø³Ø§Ù„Ù… Ù…Ù…Ù†ÙˆÙ† Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§',\n",
        "    'Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒÚ©Ù†Ù… Ø­ØªÙ…Ø§ Ø®Ø±ÛŒØ¯ Ú©Ù†ÛŒØ¯ Ùˆ Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ù… Ù…Ø«Ù„ Ø§Ù„Ù…Ø§Ø³ ØªÙˆ Ø¯Ø³ØªØªÙˆÙ† Ø¨Ø¯Ø±Ø®Ø´Ù‡ØŒğŸ‘ŒğŸ‘',\n",
        "    'ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø³Øª Ø§Ù¾Ù„',\n",
        "    'ØªÙˆ Ú¯ÙˆØ´ÛŒÙ‡Ø§ÛŒ Ø¢ÛŒÙÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯ Ùˆ Ø±ÛŒØ¬Ø³ØªØ± Ø´Ø¯Ù‡ Ø¨Ù‡ Ø±ÙˆØ²ØªØ±ÛŒÙ†Ù‡ Ùˆ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø±Ùˆ Ø¨Ø±Ø¢ÙˆØ±Ø¯Ù‡ Ù…ÛŒ Ú©Ù†Ù‡.',\n",
        "    'Ø§Ù…Ø±ÙˆØ² Ø±Ø³ÛŒØ¯ Ø¯Ø³ØªÙ… Ø®ÛŒÙ„ÛŒ Ø¹Ø§Ù„ÛŒ Ø²ÛŒØ¨Ø§ Ø®ÙˆØ´ Ø¯Ø³Øª Ø­Ø³ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ÛŒ Ø±Ùˆ Ø¨Ù‡ Ø¢Ø¯Ù… Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒÚ©Ù†Ù‡ Ø®ØµÙˆØµØ§ Ø§Ú¯Ù‡ Ù…Ø«Ù„ Ù…Ù† Ø®ÛŒÙ„ÛŒ ÙˆÙ‚ØªÙ‡ Ø°ÙˆÙ‚ Ø¯Ø§Ø´ØªÙ†Ø´Ùˆ Ø¯Ø§Ø±ÛŒØ¯ Ø¸Ø§Ù‡Ø± Ø±Ù†Ú¯ Ø³ÙÛŒØ¯ Ø§Ø³ØªØ§Ø±Ù„Ø§ÛŒØªØ´ ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø¬Ø°Ø§Ø¨Ù‡ Ù…Ù† Ø¨ÛŒÙ† Ø§ÛŒÙ† Ø±Ù†Ú¯ Ùˆ ØµÙˆØ±ØªÛŒ Ù…ÙˆÙ†Ø¯Ù‡ Ø¨ÙˆØ¯Ù… Ùˆ Ø®ÙˆØ´Ø­Ø§Ù„Ù… Ø§ÛŒÙ† Ø±Ù†Ú¯ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯Ù… ÛŒÚ©Ù… ÙˆØ§Ø³Ù‡ Ø¨Ø­Ø« Ø±Ø¬ÛŒØ³ØªØ±ÛŒØ´ Ù…ÛŒØªØ±Ø³ÛŒØ¯Ù… ÙˆØ§Ø³Ù‡ Ù‡Ù…ÛŒÙ† Ø®ÛŒÙ„ÛŒ Ù…Ø±Ø¯Ø¯ Ø¨ÙˆØ¯Ù… Ø§Ø² Ú©Ø¯ÙˆÙ… ÙØ±ÙˆØ´Ù†Ø¯Ù‡ Ø¨Ú¯ÛŒØ±Ù… Ú¯Ø§Ø±Ø§Ù†ØªÛŒØ´ Ø§ÙˆÚ©ÛŒ Ø¨Ø§Ø´Ù‡ . Ù¾Ø§ÛŒØ§Ú¯Ø³ØªØ± Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯Ù… Ø¨Ø§ Ú¯Ø§Ø±Ø§Ù†ØªÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ú˜ÛŒÙˆØ§Ù† Ù‚Ø¨Ù„Ø´ Ø³Ø±Ú† Ú©Ø±Ø¯Ù… Ø³Ø§ÛŒØªØ´ÙˆÙ† Ø§ÙˆÚ©ÛŒ Ø¨ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒ Ù…Ø´ØªØ±ÛŒ Ù‡Ø§Ø´ÙˆÙ† Ø¨ÙˆØ¯Ù†. Ùˆ Ø®Ø¯Ø§Ø±ÙˆØ´Ú©Ø± Ø±Ø¬ÛŒØ³ØªØ± Ø´Ø¯ . Ø´Ù…Ø§Ø±Ù‡ Ú¯Ø§Ø±Ø§Ù†ØªÛŒ Ø±Ùˆ ØªÙˆÛŒ Ø³Ø§ ...',\n",
        "    'Ù…Ø´Ú©Ù„ ÙÛŒØ³ Ø¢ÛŒØ¯ÛŒ Ø¯Ø§Ø´Øª Ø§ØµÙ„Ø£ Ø§Ø² Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§ Ú¯ÙˆØ´ÛŒ Ù†Ø®Ø±ÛŒØ¯ Ø¨Ø§ Ø®ÛŒØ§Ù„ Ø±Ø§Ø­Øª Ø¨Ø±ÛŒØ¯ Ø­Ø¶ÙˆØ±ÛŒ Ø®Ø±ÛŒØ¯ Ú©Ù†ÛŒØ¯',\n",
        "    'Ø®Ø±ÛŒØ¯ Ø§ÛŒÙ† Ú¯ÙˆØ´ÛŒ Ø±Ø§ Ø¨Ø§ ØªÙˆÚ†Ù‡ Ø¨Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø¹Ø§Ù„ÛŒ Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØªØ´ ØŒ Ø³ÛŒ Ù¾ÛŒ ÛŒÙˆ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯Ø´ØŒ Ø¨Ø§ØªØ±ÛŒ Ùˆ Ø´Ø§Ø±Ú˜Ø¯Ù‡ÛŒ Ù…Ù†Ø§Ø³Ø¨Ø´ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù†Ù…Ø§ÛŒØ´Ú¯Ø± Ø¨Ø§Ú©ÛŒÙÛŒØª Ùˆ Ø·Ø±Ø§Ø­ÛŒ Ø²ÛŒØ¨Ø§ÛŒ Ø¢Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒÚ©Ù†Ù…',\n",
        "    'Ù…Ù…Ù†ÙˆÙ† Ø¨Ø§Ø¨Øª ÙˆÙ‚Øª Ø´Ù†Ø§Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ Ùˆ Ø§Ù„Ø¨ØªÙ‡ Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù¾Ù„Ù…Ù¾ Ùˆ Ø³Ø§Ù„Ù… Ø¨ÙˆØ¯Ù† Ø¯Ø³ØªÚ¯Ø§Ù‡',\n",
        "    'Ø§ÛŒÙÙˆÙ† Ø¯ÛŒÚ¯Ù‡ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ú†ÛŒØ²ÛŒ Ù†Ø¯Ø§Ø±Ù‡ Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ùˆ Ø¶Ø¹ÙØ´ Ù…Ø¹Ù„ÙˆÙ…Ù‡ ÙÙ‚Ø· Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒØªÙˆÙ†Ø³ØªÙ… Ù†Ø­ÙˆÙ‡ Ø±Ø³ÛŒØ¯Ù† Ùˆ Ù…Ø¯Øª Ø±Ø³ÛŒØ¯Ù† Ùˆ Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ø±Ùˆ Ø¨Ú¯Ù… Ú©Ù‡ Ø§ÙˆÙ†Ø§ Ù‡Ù… Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯Ù† ØªÙ‡Ø±Ø§Ù† Ø¨ÙˆØ¯Ù… Ø´Ø¨ Ø®Ø±ÛŒØ¯Ù… ÙØ±Ø¯Ø§Ø´ Ø±Ø³ÛŒØ¯ Ø¯Ø³ØªÙ…',\n",
        "    'Ø¨Ø§ Ø®ÛŒØ§Ù„ Ø±Ø§Ø­Øª Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§ Ø¨Ø®Ø±ÛŒØ¯ (Ø®ÙˆØ¯Ù… Ø¨Ø§ Ø§Ø³ØªØ±Ø³ Ú¯Ø±ÙØªÙ… ÙˆÙ„ÛŒ Ø®ÛŒÙ„ÛŒ Ø²ÙˆØ¯ Ø±Ø³ÛŒØ¯ Ùˆ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø®ÙˆØ¨ Ø¨ÙˆØ¯ Ù†Ø§Øª Ø¨ÙˆØ¯ Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨ Ø¨ÙˆØ¯ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø®Ø±ÛŒØ¯ Ú©Ù†ÛŒØ¯ Ù„Ø°Øª Ø¨Ø¨Ø±ÛŒØ¯)',\n",
        "    'Ú¯ÙˆØ´ÛŒ Ø®ÙˆØ¨ÛŒ Ù‡Ø³Øª Ø§Ù…Ø§ Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ ØªÚ©â€ŒØ³ÛŒÙ…Ú©Ø§Ø±ØªÙ‡ Ù‡Ø³ØªØŒ Ø¨Ø§ Ø§ÛŒÙ†Ú©Ù‡ ØªÙˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ±ÙˆØ´Ø´ Ù†ÙˆØ´ØªÙ‡ Ø¨ÙˆØ¯ Ø¯Ùˆâ€ŒØ³ÛŒÙ…Ú©Ø§Ø±ØªÙ‡',\n",
        "    'Ø®Ø±ÛŒØ¯ ØªÙˆ ÛŒÚ© Ø±ÙˆØ² â€ŒÙˆ ØªØ­ÙˆÛŒÙ„ Ù‡Ù… Ø¯Ø± Ù‡Ù…Ø§Ù† Ø±ÙˆØ² ÙˆØ§Ù‚Ø¹Ø§ Ø¹Ø§Ù„ÛŒ Ùˆ Ø¨ÛŒ Ù†Ù‚Øµ Ø±ÛŒØ¬Ø³ØªØ±ÛŒ Ø³Ø±ÛŒØ¹Ù«Ø®ÛŒÙ„ÛŒ Ú¯ÙˆØ´ÛŒ Ø®ÙˆØ´ Ø¯Ø³Øª Ùˆ Ø¹Ø§Ù„ÛŒ Ù‡Ø³ØªØ´',\n",
        "    'Ù„Ø·ÙØ§ Ú¯ÙˆØ´ÛŒ Ø±Ø§ Ø¯Ø± Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ù…Ø­ÙÙˆØ¸ ØªØ±ÛŒ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯ . ØªÙˆÙ„ÛŒØ¯ Û²Û°Û²Û´ Ø¨ÙˆØ¯ Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ú¯ÙˆØ´ÛŒ Ú©Ø§Ù…Ù„Ø§ Ù¾Ù„Ù…Ù¾ Ùˆ Ø³Ø§Ù„Ù… Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯',\n",
        "    'Ø¯Ù„Ø§Ø± Ú¯Ø±ÙˆÙ† Ù…ÛŒØ´Ù‡ Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡ Ù†Ù…ÛŒÚ©Ø´Ù‡ Ú¯Ø±ÙˆÙ† Ù…ÛŒÚ©Ù†Ù† Ø­Ø§Ù„Ø§ Ú©Ù‡ Ø§Ø±Ø²ÙˆÙ† Ø´Ø¯Ù‡ Ø¹ÛŒÙ† Ø®ÛŒØ§Ù„Ø´ÙˆÙ†Ù… Ù†ÛŒØ³Øª',\n",
        "    'Ú©Ø¯ ÙØ¹Ø§Ù„Ø³Ø§Ø²ÛŒ Ø±Ùˆ Ú©Ù‡ ÙˆØ§Ø±Ø¯ Ù…ÛŒÚ©Ù†Ù… Ù…ÛŒÚ¯Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ùˆ Ø±Ø¬ÛŒØ³ØªØ± Ù†Ù…ÛŒØ´Ù‡',\n",
        "    'Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ Ø¨Ù‡ Ø¯Ø³ØªÙ… Ø±Ø³ÛŒØ¯.Ù…Ù† Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ø³Ø±Ù… Ù‡Ø¯ÛŒÙ‡ Ú¯Ø±ÙØªÙ… Ø±Ù†Ú¯ Ø³Ø¨Ø² ØªÛŒØ±Ù‡ Ú©Ù‡ Ø®ÛŒÙ„ÛŒ Ù‚Ø´Ù†Ú¯Ù‡. Ù‡Ù†ÙˆØ² Ø±ÙˆØ´Ù†Ø´ Ù†Ú©Ø±Ø¯ÛŒÙ… Ø§Ù…Ø§ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø§ÙˆÚ©ÛŒ Ø¨ÙˆØ¯ Ø¸Ø§Ù‡Ø±Ø§&amp;#34;. Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø²ÙˆÙ†ØªØ± Ø¨ÙˆØ¯.',\n",
        "    'Ù‡Ù…Ù‡ Ú†ÛŒØ²ØŒØ¹Ø§Ù„ÛŒ Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù…ÙˆÙ‚Ø¹ Ù…Ù…Ù†ÙˆÙ† Ø§Ø² Ø¯Ø¨Ø¬ÛŒ Ú©Ø§Ù„Ø§ Ù…Ù…Ù†ÙˆÙ† Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ù¾ÛŒ',\n",
        "    'Ø¢ÛŒÙÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ¹Ø±ÛŒÙ Ù†Ø¯Ø§Ø±Ù‡ ÙÙ‚Ø· Ø¯ÙˆØ³ØªØ§Ù† Ø¨Ø¹Ø¯ Ø§Ø² Ø²Ø¯Ù† Ø§Ù¾Ù„ Ø¢ÛŒØ¯ÛŒ Ø­ØªÙ…Ø§ find my IPhone Ø®Ø§Ù…ÙˆØ´ Ú©Ù†ÛŒÙ† ØªØ§ Ø§Ù¾Ù„ Ø¢ÛŒØ¯ÛŒØªÙˆÙ† Ù„Ø§Ú© Ù†Ø´Ù‡ Ù…Ù† Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ù¾ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ú¯Ø±ÙØªÙ… Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨Ù‡ Ø²ÙˆØ¯ Ø§Ø¹ØªØ¨Ø§Ø± Ø¯Ø§Ø¯ Ú©Ù‡ ØªÙˆÙ†Ø³ØªÙ… Ø¨Ø®Ø±Ù… Ù…Ù…Ù†ÙˆÙ† Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ù¾ÛŒğŸ˜',\n",
        "    'Ø§Ø² Ù‡Ù…Ù‡ Ù†Ø¸Ø± Ú†Ú© Ú©Ø±Ø¯Ù… Ø§ÙˆÚ©ÛŒ Ùˆ Ù†Ø§Øª Ø§Ú©ØªÛŒÙˆ Ø¨ÙˆØ¯ Ù…Ù…Ù†ÙˆÙ† Ø§Ø² Ø¯ÛŒØ¬ÛŒ....',\n",
        "    'Ø§ÛŒÙÙˆÙ† Ø¯ÛŒÚ¯Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ¹Ø±ÛŒÙ Ù†Ø¯Ø§Ø±Ù‡ Ù…Ù…Ù†ÙˆÙ† Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ø¨Ø§Ø¨Øª Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ Ú©Ø§Ù„Ø§ ğŸ©·ğŸ‘',\n",
        "    'Ø®ÛŒÙ„ÛŒ Ø±Ø§Ø¶ÛŒ Ø§Ù… Ø§Ø² Ø®Ø±ÛŒØ¯ Ø§ÛŒÙ† Ú¯ÙˆØ´ÛŒ Ø¨Ø§ ØªØ´Ú©Ø± Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§',\n",
        "    'Ø³ÙÛŒØ¯Ø´Ù… ØªØ®ÙÛŒÙ Ø¨Ø°Ø§Ø±ÛŒØ¯ Ù„Ø·ÙØ§Ø§Ø§Ø§Ø§',\n",
        "    'Ø¨Ù‡ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§ÛŒÙ†Ú©Ù‡ Ø³Ø±ÛŒ Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ØªØ± Ø§ÛŒÙ† Ú¯ÙˆØ´ÛŒ Ø±Ø¬ÛŒØ³ØªØ± Ù†Ù…ÛŒØ´Ù‡ Ùˆ Ù…Ù…Ú©Ù†Ù‡ Ø§Ø² Ø¨ÛŒØ±ÙˆÙ† Ù†ØªÙˆÙ†ÛŒÙ† Ù¾Ú© Ø§ØµÙ„ÛŒ Ø±Ùˆ Ø¨Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø®Ø±ÛŒØ¯ØŒ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ù‡ Ù…Ù‡Ù„Øª Ù…Ø±Ø¬ÙˆØ¹ÛŒ Ù…ÛŒØªÙˆÙ†ÛŒÙ† Ø®Ø±ÛŒØ¯ØªÙˆÙ† Ø±Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒØ¯. Ø¯Ø±Ø¶Ù…Ù† Ø¨Ø±Ø§ÛŒ Ø®Ø±ÛŒØ¯ Ú¯ÙˆØ´ÛŒ Ø¢ÛŒÙÙˆÙ† Ú©Ù‡ Ø±Ø¬ÛŒØ³ØªØ± Ø¨Ø´Ù‡ØŒ Ú¯Ø²ÛŒÙ†Ù‡ ÛŒ Ø¯ÛŒÚ¯Ù‡ Ø§ÛŒ Ù‡Ù… Ù†ÛŒØ³Øª Ú©Ù‡ Ø¨Ø®ÙˆØ§ÛŒÙ† Ø¨Ù‡Ø´ ÙÚ©Ø± Ú©Ù†ÛŒÙ† Ùˆ Ù‡Ù…ÛŒÙ† ØªÙ†Ù‡Ø§ Ú¯Ø²ÛŒÙ†Ù‡ Ù…ÙˆØ¬ÙˆØ¯Ù‡. Ù¾Ø³ ØªØ§ Ú¯Ø±ÙˆÙ†ØªØ± Ù†Ø´Ø¯Ù‡ ÛŒØ§ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù¾Ú© Ø§ØµÙ„ÛŒ ØªÙ…Ø§Ù… Ù†Ø´Ø¯Ù‡ØŒ Ø®Ø±ÛŒØ¯ØªÙˆÙ† Ø±Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒØ¯. Ø¯Ø±Ù…ÙˆØ±Ø¯ Ø®ÙˆØ¯ Ú¯ÙˆØ´ÛŒ Ù‡Ù… Ø­Ø±ÙÛŒ Ø¨Ø±Ø§ÛŒ Ú¯ÙØªÙ† Ù†ÛŒØ³Øª Ùˆ Ú¯ÙˆØ´ÛŒ Ø­Ø±Ù Ù†Ø¯Ø§Ø±Ù‡!',\n",
        "    'Ø¯Ø± Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§ Ù‚ÛŒÙ…Øª Ø®ÛŒÙ„ÛŒ Ù…Ù†Ø§Ø³Ø¨ØªØ± Ø§Ø³Øª',\n",
        "    'Ø¨Ù‡ Ø´Ø®ØµÙ‡ Ø¨Ø§ Ø§ÛŒÙ† Ø·Ø±Ø§Ø­ÛŒ Ø§ÛŒÙÙˆÙ† Û±Û³ Ø¨ÛŒØ´ØªØ± Ø­Ø§Ù„ Ù…ÛŒÚ©Ù†Ù… ØªØ§ Û±Ûµ Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø¨Ù‡ Ø®ÙˆØ¯Ù… Ø¯ÛŒØ¬ÛŒ Ù¾ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ù†Ù…ÛŒØ¯Ø§Ø¯ Ø¨Ø§ Ø­Ø³Ø§Ø¨ Ù¾Ø¯Ø±Ù… Ú¯Ø±ÙØªÙ…! Ú©Ø§Ø´ Ù…ÛŒØ´Ø¯ Ø§Ø¹ØªØ¨Ø§Ø± Ø±Ùˆ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø§Ø¯ ØªØ§ Ø¨ØªÙˆÙ†Ù… Ø­Ø¯Ø§Ù‚Ù„ Ø¨Ø§ Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø®ÙˆØ¯Ù… Ø¨Ú¯ÛŒØ±Ù… Ø§Ø² Ø§ÛŒÙ† ÙØ±ÙˆØ´Ù†Ø¯Ù‡ Ù‡Ù… Ø±Ø§Ø¶ÛŒÙ… Ù…Ø´Ú©Ù„ÛŒ ØªÙˆ Ú©Ø§Ø± Ù†Ø¨ÙˆØ¯',\n",
        "    'Ø§Ø² Ù†Ø¸Ø± Ù…Ù† Ú¯ÙˆØ´ÛŒ Ø®ÙˆØ¨Ù‡â€ŒØ§ÛŒÙ‡ ÙˆÙ„ÛŒ Ø§Ø±Ø²Ø´ Ø®Ø±ÛŒØ¯ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø§ÛŒÙ† Ù‚ÛŒÙ…Øª Ø±Ùˆ Ù†Ù…ÛŒâ€ŒØ¯ÙˆÙ†Ù… Ø¯Ø§Ø±Ù‡ ÛŒØ§ Ù†Ù‡ Ù…Ø«Ù„Ø§ ÙˆÛŒØ¯ÛŒÙˆ Ù‡Ø§ÛŒ Ø§ÙØ±Ø§Ø¯ Ø±Ùˆ Ø¨Ø¨ÛŒÙ†ÛŒØ¯ Ùˆ Ø¨Ø¹Ø¯ Ø®Ø±ÛŒØ¯ Ú©Ù†ÛŒØ¯ ÛŒÙ‡ Ø¹Ø§Ù„Ù… ÙˆÛŒÚ˜Ú¯ÛŒ Ø®ÙˆØ¨ Ø¯Ø§Ø±Ù‡ Ù‡Ø§ ÙˆÙ„ÛŒ Ø²ÙˆØ¯ Ø¯Ø§Øº Ù…ÛŒÚ©Ù†Ù‡ ',\n",
        "    'Ø¨Ø¹Ø¯ Ø§Ø² Ø´Ø´ Ù…Ø§Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ùˆ Ø±Ø¹Ø§ÛŒØª Ù†Ú©Ø§ØªÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÙ‡ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡Ø´ ØªÙˆØ¬Ù‡ Ø¨Ø´Ù‡ ØŒ Ø³Ù„Ø§Ù…Øª Ø¨Ø§ØªØ±ÛŒ Ø¨Ù‡ Û¹Û± Ø¯Ø±ØµØ¯ Ø±Ø³ÛŒØ¯Ù‡ Ùˆ Ø¨Ù‡ Ø´Ø¯Øª Ù‡Ù…ÛŒØ´Ù‡ Ù…ÙˆÙ‚Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø§Øº Ù…ÛŒÚ©Ù†Ù‡ ÙˆØ§Ù‚Ø¹Ø§ Ù†Ù…ÛŒØ¯ÙˆÙ†Ù… Ø§ÛŒÙÙˆÙ† Ù‚ÛŒÙ…Øª Ø¨Ø§Ù„Ø§ÛŒÛŒ Ù†Ø¯Ø§Ø±Ø¯',\n",
        "    'Ø³Ù„Ø§Ù… Ù…Ù…Ù†ÙˆÙ†Ù… Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§ Ø§ÛŒÙ† Ø³ÙˆÙ…ÛŒÙ† Ú¯ÙˆØ´ÛŒ Ø¨ÙˆØ¯ Ú© Ø®Ø±ÛŒØ¯Ø§Ø±ÛŒ Ú©Ø±Ø¯Ù… Ø§Ø² Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§ ÙˆÙ„ÛŒ Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§ÛŒÙ†Ø¨Ø§Ø± Ù…Ø´Ø®ØµØ§Øª Ù†Ø§Ù‚Øµ Ø¨ÙˆØ¯ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø³ÛŒÙ…Ú©Ø§Ø±Øª Ø²Ø¯ Ø¨ÙˆØ¯Ù† Ø¯Ùˆ Ø³ÛŒÙ…Ú©Ø§Ø±Øª',\n",
        "    'Ù…Ù† Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø§Ø¹ØªØ¨Ø§Ø± Ø¯ÛŒØ¬ÛŒ Ù¾ÛŒ Ø®Ø±ÛŒØ¯Ù… Ùˆ ÙˆØ§Ù‚Ø¹Ø§ ÙˆØ§Ø³Ù… Ø¨ØµØ±ÙÙ‡ ØªØ± Ø§Ø² Ø®Ø±ÛŒØ¯ Ù‚Ø³Ø·ÛŒ Ø§Ø² Ù…ØºØ§Ø²Ù‡ Ø¨ÙˆØ¯Ù« Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ù…Ø´Ú©Ù„ÛŒ Ù†Ø¯Ø§Ø´Øª Ùˆ Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ Ø¨Ø¯Ø³ØªÙ… Ø±Ø³ÛŒØ¯ Ù«Ù…Ø´Ø®ØµØ§Øª ØªÙˆ Ø³Ø§ÛŒØª Ú†Ú© Ú©Ø±Ø¯Ù… Ùˆ Ø³Ø±ÛŒ Ø§ØµÙ„ÛŒ Ø¨ÙˆØ¯Ù« Ù…Ù† Ø±Ù†Ú¯ Ù…Ø´Ú©ÛŒ Ø®Ø±ÛŒØ¯Ù… ÙˆÙ„ÛŒ Ø±Ù†Ú¯ Ø³ÙÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒÚ©Ù†Ù… Ùˆ Ø®ÙˆØ¯Ù… Ø¨Ø±Ú¯Ø±Ø¯Ù… Ø¹Ù‚Ø¨ Ø³ÙÛŒØ¯ Ù…ÛŒÚ¯ÛŒØ±Ù…!',\n",
        "\n",
        "    'Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ù†Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡ Ùˆ ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª 50 Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡ Ùˆ Ø±Ø§Ø¶ÛŒØªÙˆÙ† Ù…ÛŒÚ©Ù†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡ Ø§Ø´ Ø§ØµÙ„Ø§ Ù‡Ù†Ú¯ Ù†Ù…ÛŒÚ©Ù†Ù‡ Ùˆ Ø¯Ø§Øº Ù†Ù…ÛŒØ´Ù‡ Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡ ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´Ø´ Ø¨Ø§ 900 Ù†ÛŒØª Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„Ø§ ØªØµØ§ÙˆÛŒØ± Ø±Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ùˆ ÙˆØ§Ø¶Ø¹ Ù†Ø´ÙˆÙ† Ù…ÛŒØ¯Ù‡. Ø¸Ø§Ù‡Ø± Ú©ÙˆØ´ÛŒ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø´ÛŒÚ© Ùˆ Ø®ÙˆØ´Ú¯Ù„Ù‡. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø­Ø°Ø§Ø¨ÛŒ Ø¯Ø§Ø±Ù‡. Ø¯Ø± Ú©Ù„ Ú¯ÙˆØ´ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ Ø®ÛŒÙ„ÛŒ Ø§Ø±Ø²ÙˆÙ†Ù‡.'\n",
        "]"
      ],
      "metadata": {
        "id": "P2YED8SML1S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = hazm.Normalizer()\n",
        "lemmatizer = hazm.Lemmatizer()\n",
        "stemmer = hazm.Stemmer()\n",
        "tagger = hazm.POSTagger(model='pos_tagger.model')\n",
        "parser = hazm.DependencyParser(tagger = tagger, lemmatizer = lemmatizer)"
      ],
      "metadata": {
        "id": "bZa-X56TRhmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def persian_sentence_tokenizer(text):\n",
        "  # Define the regex pattern for sentence splitting\n",
        "  conjunction_words = r\"\\s+(?:Ùˆ|Ø§Ù…Ø§|ÙˆÙ„ÛŒ|ÙˆÙ„ÛŒÚ©Ù†)\\s+\"\n",
        "  pattern = f\"{conjunction_words}|[.ØŸ!]+\"\n",
        "\n",
        "  # Split the text using the pattern\n",
        "  sentences = re.split(pattern, text)\n",
        "\n",
        "  # Remove empty sentences\n",
        "  sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "  return sentences\n",
        "\n",
        "\n",
        "def split_sentence_on_verb(sentence):\n",
        "    # Tokenize and tag the sentence\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tagged_tokens = tagger.tag(tokens)\n",
        "\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "\n",
        "    for word, tag in tagged_tokens:\n",
        "        current_sentence.append(word)\n",
        "        if tag == 'VERB':\n",
        "            sentences.append(' '.join(current_sentence))\n",
        "            current_sentence = []\n",
        "\n",
        "    # Append the last accumulated sentence if any\n",
        "    if current_sentence:\n",
        "        sentences.append(' '.join(current_sentence))\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def persian_sentence_tokenizer_v3(text):\n",
        "    def contains_verbs(tags):\n",
        "        counter = 0\n",
        "        for tag in tags:\n",
        "            if tag in ['VERB']:\n",
        "                counter += 1\n",
        "                if counter>1:\n",
        "                  return True\n",
        "        return False\n",
        "\n",
        "    def contains_noun_or_ez(tagged_tokens):\n",
        "        for word, tag in tagged_tokens:\n",
        "            if 'NOUN' in tag:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    tagger = hazm.POSTagger(model = 'pos_tagger.model')\n",
        "    sentences = []\n",
        "    for sentence in sent_tokenize(text):\n",
        "        tokens = word_tokenize(sentence)\n",
        "        tagged_tokens = tagger.tag(tokens)\n",
        "        tags = [tag for _, tag in tagged_tokens]\n",
        "        if contains_verbs(tags):\n",
        "            temp_sentences = split_sentence_on_verb(sentence)\n",
        "            for temp_sentence in temp_sentences:\n",
        "                temp_tokens = word_tokenize(temp_sentence)\n",
        "                temp_tagged_tokens = tagger.tag(temp_tokens)\n",
        "                if contains_noun_or_ez(temp_tagged_tokens):\n",
        "                    # print(temp_tagged_tokens)\n",
        "                    sentences.append(temp_sentence)\n",
        "                else:\n",
        "                    if sentences[-1]:\n",
        "                        sentences[-1] += (\" Ùˆ \"+temp_sentence)\n",
        "                    else:\n",
        "                        sentences.append(temp_sentence)\n",
        "        else:\n",
        "          sentences.append(sentence)\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "YoWPVf_-roYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "sentence = sent_tokenize(comments[50])[1]\n",
        "split_sentences = split_sentence_on_verb(sentence)\n",
        "print(split_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjH87zAP-lHn",
        "outputId": "92332362-0666-4839-89f6-1941a8884d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡', 'Ùˆ ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª 50 Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡', 'Ùˆ Ø±Ø§Ø¶ÛŒØªÙˆÙ† Ù…ÛŒÚ©Ù†Ù‡', 'Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡ Ø§Ø´ Ø§ØµÙ„Ø§ Ù‡Ù†Ú¯ Ù†Ù…ÛŒÚ©Ù†Ù‡', 'Ùˆ Ø¯Ø§Øº Ù†Ù…ÛŒØ´Ù‡', 'Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡', 'ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´Ø´ Ø¨Ø§ 900 Ù†ÛŒØª Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„Ø§ ØªØµØ§ÙˆÛŒØ± Ø±Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ùˆ ÙˆØ§Ø¶Ø¹ Ù†Ø´ÙˆÙ† Ù…ÛŒØ¯Ù‡', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sent = persian_sentence_tokenizer(comments[50])\n",
        "tokenized_sent1 = persian_sentence_tokenizer_v3(comments[50])"
      ],
      "metadata": {
        "id": "CTlw0ZSJ3JSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzfKbged3m86",
        "outputId": "966f8c8c-32dd-4e7c-edfb-434a49228edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ù†Ø¯Ù‡ Ø§Ø³Øª',\n",
              " 'Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡',\n",
              " 'ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª 50 Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡',\n",
              " 'Ø±Ø§Ø¶ÛŒØªÙˆÙ† Ù…ÛŒÚ©Ù†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡ Ø§Ø´ Ø§ØµÙ„Ø§ Ù‡Ù†Ú¯ Ù†Ù…ÛŒÚ©Ù†Ù‡',\n",
              " 'Ø¯Ø§Øº Ù†Ù…ÛŒØ´Ù‡',\n",
              " 'Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡ ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´Ø´ Ø¨Ø§ 900 Ù†ÛŒØª Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„Ø§ ØªØµØ§ÙˆÛŒØ± Ø±Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØª',\n",
              " 'ÙˆØ§Ø¶Ø¹ Ù†Ø´ÙˆÙ† Ù…ÛŒØ¯Ù‡',\n",
              " 'Ø¸Ø§Ù‡Ø± Ú©ÙˆØ´ÛŒ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø´ÛŒÚ©',\n",
              " 'Ø®ÙˆØ´Ú¯Ù„Ù‡',\n",
              " 'Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø±ÙˆØ§Ù†',\n",
              " 'Ø­Ø°Ø§Ø¨ÛŒ Ø¯Ø§Ø±Ù‡',\n",
              " 'Ø¯Ø± Ú©Ù„ Ú¯ÙˆØ´ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª',\n",
              " 'Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ Ø®ÛŒÙ„ÛŒ Ø§Ø±Ø²ÙˆÙ†Ù‡']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sent1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR-_lR3b33ZD",
        "outputId": "57b7a32a-d63e-424b-c7c3-bd0b01ad4a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ù†Ø¯Ù‡ Ø§Ø³Øª.',\n",
              " 'Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡',\n",
              " 'Ùˆ ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª 50 Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡',\n",
              " 'Ùˆ Ø±Ø§Ø¶ÛŒØªÙˆÙ† Ù…ÛŒÚ©Ù†Ù‡',\n",
              " 'Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡ Ø§Ø´ Ø§ØµÙ„Ø§ Ù‡Ù†Ú¯ Ù†Ù…ÛŒÚ©Ù†Ù‡ Ùˆ Ùˆ Ø¯Ø§Øº Ù†Ù…ÛŒØ´Ù‡',\n",
              " 'Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡',\n",
              " 'ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´Ø´ Ø¨Ø§ 900 Ù†ÛŒØª Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„Ø§ ØªØµØ§ÙˆÛŒØ± Ø±Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ùˆ ÙˆØ§Ø¶Ø¹ Ù†Ø´ÙˆÙ† Ù…ÛŒØ¯Ù‡ Ùˆ .',\n",
              " 'Ø¸Ø§Ù‡Ø± Ú©ÙˆØ´ÛŒ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø´ÛŒÚ© Ùˆ Ø®ÙˆØ´Ú¯Ù„Ù‡.',\n",
              " 'Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø­Ø°Ø§Ø¨ÛŒ Ø¯Ø§Ø±Ù‡.',\n",
              " 'Ø¯Ø± Ú©Ù„ Ú¯ÙˆØ´ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ Ø®ÛŒÙ„ÛŒ Ø§Ø±Ø²ÙˆÙ†Ù‡.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SmartPhoneFeatureExtractor Class**"
      ],
      "metadata": {
        "id": "ruDAGw8eZXcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_battery_sentences(sent):\n",
        "    # Keywords related to smartphone battery in Persian\n",
        "    battery_keywords = r'(Ø¨Ø§ØªØ±ÛŒ|Ø¨Ø§Ø·Ø±ÛŒ|Ø´Ø§Ø±Ú˜|Ø´Ø§Ø±Ú˜Ø¯Ù‡ÛŒ|Ø´Ø§Ø±Ú˜ Ø³Ø±ÛŒØ¹|Ø´Ø§Ø±Ú˜ Ø¨ÛŒâ€ŒØ³ÛŒÙ…|Ø´Ø§Ø±Ú˜Ø±|Ù¾Ø§ÙˆØ±Ø¨Ø§Ù†Ú©)'\n",
        "\n",
        "    # Regex pattern to match sentences containing battery-related keywords\n",
        "    pattern = r'\\b(' + battery_keywords + r')\\b'\n",
        "\n",
        "    return re.search(pattern, sent) is None\n",
        "\n",
        "# Example usage\n",
        "persian_comments = \"\"\"\n",
        "Ú¯ÙˆØ´ÛŒ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª. Ø¨Ø§ØªØ±ÛŒ Ø¢Ù† Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ Ø§Ø³Øª Ùˆ Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø´Ø§Ø±Ú˜ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±Ø¯.\n",
        "Ø§Ù…Ø§ Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ú¯ÙˆØ´ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±ØŒ Ø¹Ù…Ø± Ø¨Ø§ØªØ±ÛŒ Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯.\n",
        "Ø´Ø§Ø±Ú˜Ø± Ø³Ø±ÛŒØ¹ Ù‡Ù…Ø±Ø§Ù‡ Ø¯Ø³ØªÚ¯Ø§Ù‡ Ø¨Ø³ÛŒØ§Ø± Ù…ÙÛŒØ¯ Ø§Ø³Øª.\n",
        "\"\"\"\n",
        "\n",
        "extracted_sentences = extract_battery_sentences(tokenized_sent[1])\n"
      ],
      "metadata": {
        "id": "zzJxFLHwnwWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extracted_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNLSElPG33Nr",
        "outputId": "17700697-a07e-4d23-fa8a-09296211fa02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please explain by example the functionality of these functions and what are they doing? Explain completely and step by step"
      ],
      "metadata": {
        "id": "zf6CrYI18E_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/parsi.io/parsi_io/modules/product_feature_extractor/data'\n",
        "\n",
        "class MyNormalizer(hazm.Normalizer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.aug_data_init()\n",
        "\n",
        "    def aug_data_init(self):\n",
        "        f = open(f'{data_path}/augmented_data.txt', 'r')\n",
        "        self.aug_data = f.readlines()\n",
        "        f.close()\n",
        "\n",
        "    def replace_with_persian(self, sent):\n",
        "        replace_list = {'Ù‰':'ÛŒ',\n",
        "                        'Ø©':'Ù‡',\n",
        "                        'ÙŠ':'ÛŒ'}\n",
        "\n",
        "        for x in replace_list:\n",
        "            sent = sent.replace(x, replace_list[x])\n",
        "\n",
        "        return sent\n",
        "\n",
        "\n",
        "    def my_normalize(self, text):\n",
        "        text = self.normalize(text)\n",
        "\n",
        "        aug = {y[0]:y[1] for y in [x.replace('\\n', '').split('\\t') for x in self.aug_data]}\n",
        "\n",
        "        for k in aug:\n",
        "            while k in text:\n",
        "                text = text.replace(k, aug[k])\n",
        "        while '\\u200c' in text:\n",
        "            text = text.replace('\\u200c', ' ')\n",
        "        return text"
      ],
      "metadata": {
        "id": "eXOoZIzISmrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmartPhoneFeatureExtractor():\n",
        "    def __init__(self):\n",
        "        self.normalizer = MyNormalizer()\n",
        "\n",
        "    def persian_sentence_tokenizer(self, text):\n",
        "      # Define the regex pattern for sentence splitting\n",
        "      conjunction_words = r\"\\s+(?:Ùˆ|Ø§Ù…Ø§|ÙˆÙ„ÛŒ|ÙˆÙ„ÛŒÚ©Ù†)\\s+\"\n",
        "      pattern = f\"{conjunction_words}|[.ØŸ!]+\"\n",
        "\n",
        "      # Split the text using the pattern\n",
        "      sentences = re.split(pattern, text)\n",
        "\n",
        "      # Remove empty sentences\n",
        "      sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "      return sentences\n",
        "\n",
        "\n",
        "    def sent_normalize(self, sent):\n",
        "        return self.normalizer.my_normalize(sent)\n",
        "\n",
        "\n",
        "    def clean_result(self, text):\n",
        "        for i in range(len(text)):\n",
        "            for base_form in self.quality_base_forms:\n",
        "                text[i] = re.sub(fr\"({base_form[0]})\", f\"{base_form[1]}\", text[i])\n",
        "        return text\n",
        "\n",
        "\n",
        "    def find_words(self, sent, pattern):\n",
        "        matchs = []\n",
        "        for m in re.finditer(pattern, sent):\n",
        "            text = m.group()\n",
        "            sp = list(m.span())\n",
        "            if text[0] == ' ':\n",
        "                sp[0] += 1\n",
        "                text = text[1:]\n",
        "            if text[-1] == ' ':\n",
        "                sp[-1] -= 1\n",
        "                text = text[:-1]\n",
        "            matchs.append([text, sp])\n",
        "        return matchs\n",
        "\n",
        "    def extract_value(self, d: dict):\n",
        "        result = None\n",
        "        for key, value in d.items():\n",
        "            if key != 'not' and value is not None:\n",
        "                result = key\n",
        "            if key == 'not' and value is not None:\n",
        "                result = self.size_not_mapper[result]\n",
        "        return result\n",
        "\n",
        "\n",
        "    def general_opinion(self, text):\n",
        "\n",
        "        noun_phrases = [\n",
        "            r'Ø¨Ù†Ø¸Ø±Ù…',\n",
        "            r'Ø¨Ù‡ Ù†Ø¸Ø± Ù…Ù†',\n",
        "            r'Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒØ±Ø³Ù‡',\n",
        "            r'Ø¨Ù†Ø¸Ø±',\n",
        "            r'Ø¨Ù‡ Ù†Ø¸Ø±',\n",
        "            r'Ø¨Ù‡ Ù†Ø³Ø¨Øª',\n",
        "            r'Ù†Ø³Ø¨Øª  Ø¨Ù‡ Ø¨Ù‚ÛŒÙ‡',\n",
        "            r'Ù‡Ù…Ù‡ Ú†ÛŒØ²',\n",
        "            r'Ù‡Ù…Ù‡â€ŒÚ†ÛŒØ²',\n",
        "            r'Ø¯Ø± Ú©Ù„',\n",
        "            r'ÙˆØ§Ù‚Ø¹Ø§',\n",
        "            r'Ù†Ø³Ø¨Øª  Ø¨Ù‡ Ø¨Ù‚ÛŒÙ‡',\n",
        "            r'Ø§Ø² Ù‡Ù…Ù‡ Ù†Ø¸Ø±',\n",
        "            r'Ú©Ø§Ù…Ù„Ø§',\n",
        "        ]\n",
        "\n",
        "        demonstrative_adj = [\n",
        "            r'Ø§ÛŒÙ†',\n",
        "            r'Ø¢Ù†',\n",
        "            r'Ø§ÙˆÙ†',\n",
        "            r'Ø§Ù†',\n",
        "        ]\n",
        "        device_keywords = r\"\\b(Ú¯ÙˆØ´ÛŒ|ØªÙ„ÙÙ† Ù‡Ù…Ø±Ø§Ù‡|ØªÙ„ÙÙ†|Ù…ÙˆØ¨Ø§ÛŒÙ„)\\b\"\n",
        "\n",
        "        device_postfix = [\n",
        "            r'Ø§',\n",
        "            r'\\u200cÙ‡Ø§',\n",
        "            # r'Ø§',\n",
        "            # r'Ø§',\n",
        "            # r'Ø§',\n",
        "        ]\n",
        "\n",
        "        pos_adj = [\n",
        "            r'Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨',\n",
        "            r'Ø®ÛŒÙ„ÛŒ Ø¹Ø§Ù„ÛŒ',\n",
        "            r'Ø¹Ø§Ù„ÛŒ',\n",
        "            r'ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡',\n",
        "            r'Ø¨Ù‡ØªØ± Ø§Ø² Ø§Ù†ØªØ¸Ø§Ø±Ù…',\n",
        "            r'Ø¨ÛŒâ€ŒÙ†Ø¸ÛŒØ±',\n",
        "            r'Ø®ÙˆØ¨',\n",
        "            r'Ø±Ø§Ø¶ÛŒ Ú©Ù†Ù†Ø¯Ù‡',\n",
        "            r'Ù…Ù†Ø§Ø³Ø¨',\n",
        "            r'Ø¯Ø±Ø¬Ù‡ ÛŒÚ©',\n",
        "        ]\n",
        "\n",
        "        pos_sent = [\n",
        "            r'Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯',\n",
        "            r'Ø±Ø¶Ø§ÛŒØª Ø¯Ø§Ø´Øª',\n",
        "            # r'',\n",
        "        ]\n",
        "        verb_identifier1 = r\"Ù‡|ÛŒÙ‡\"\n",
        "\n",
        "        verb_identifier2 = [\n",
        "            r\"(Ù…|\\u200cÙ…|)\",\n",
        "            r\"(Ù†Ø¯|\\u200cØ§Ù†Ø¯)\",\n",
        "        ]\n",
        "\n",
        "        neg_adj = [\n",
        "            r'Ø®ÛŒÙ„ÛŒ Ø¨Ø¯',\n",
        "            r'Ø¨Ø¯',\n",
        "            r'Ø¨Ø¯',\n",
        "            r'Ø§ÙØªØ¶Ø§Ø­',\n",
        "            r'Ù†Ø§Ø§Ù…ÛŒØ¯ Ú©Ù†Ù†Ø¯Ù‡',\n",
        "            r'Ø¶Ø¹ÛŒÙ',\n",
        "            r'Ù…ØªÙˆØ³Ø·',\n",
        "            r'Ù…Ø¹Ù…ÙˆÙ„ÛŒ',\n",
        "            r'Ù†Ø§Ù…Ù†Ø§Ø³Ø¨',\n",
        "        ]\n",
        "\n",
        "        adj_postfix = [\n",
        "            r'ÛŒ',\n",
        "            r'Ø§ÛŒ',\n",
        "            r' Ø§ÛŒ',\n",
        "            r'\\u200cØ§ÛŒ'\n",
        "\n",
        "        ]\n",
        "\n",
        "        pos_verb = [\n",
        "            r'Ø¨ÙˆØ¯',\n",
        "            r'Ù‡Ø³Øª',\n",
        "            r'Ø§Ø³Øª',\n",
        "        ]\n",
        "        neg_verb = [\n",
        "            r'Ù†Ø¨ÙˆØ¯',\n",
        "            r'Ù†ÛŒØ³Øª',\n",
        "        ]\n",
        "\n",
        "        # Ø³Ø§Ø®Øª Ù¾ØªØ±Ù† Ù‡Ø§ÛŒ Ù†Ø¸Ø±Ø§Øª Ù…Ø«Ø¨Øª\n",
        "        pos_pattern1 = rf\"(?:{'|'.join(noun_phrases)})?{demonstrative_adj}*{device_keywords}{device_postfix}*.*?\\b({'|'.join(pos_adj)})(?:{'|'.join(adj_postfix)})?\\b.*?\\b({'|'.join(pos_verb)})\\b\"\n",
        "        pos_pattern2 = rf\"(?:{'|'.join(noun_phrases)})?{demonstrative_adj}.*?\\b({'|'.join(pos_adj)})(?:{'|'.join(adj_postfix)})?\\b.*?\\b({'|'.join(pos_verb)})\\b\"\n",
        "        pos_pattern3 = rf\"(?:{'|'.join(noun_phrases)})?.*?\\b({'|'.join(pos_adj)})(?:{'|'.join(adj_postfix)})?\\b.*?\\b({'|'.join(pos_verb)})\\b\"\n",
        "        pos_pattern4 = rf\"{'|'.join(noun_phrases)}.*({'|'.join(pos_adj)})(?:{verb_identifier1})?\"\n",
        "        pos_pattern5 = rf\"(?:{'|'.join(noun_phrases)})?.*{device_keywords}{device_postfix}*.*?\\b({'|'.join(pos_sent)})(?:{'|'.join(verb_identifier2)})?\\b.*\"\n",
        "\n",
        "        # Ø³Ø§Ø®Øª Ù¾ØªØ±Ù† Ù‡Ø§ÛŒ Ù†Ø¸Ø±Ø§Øª Ù…Ù†ÙÛŒ\n",
        "        neg_pattern1 = rf\"(?:{'|'.join(noun_phrases)})?{demonstrative_adj}*{device_keywords}{device_postfix}*.*?\\b({'|'.join(neg_adj)})(?:{'|'.join(adj_postfix)})?\\b.*?\\b({'|'.join(pos_verb)})\\b\"\n",
        "        neg_pattern2 = rf\"(?:{'|'.join(noun_phrases)})?{demonstrative_adj}.*?\\b({'|'.join(neg_adj)})(?:{'|'.join(adj_postfix)})?\\b.*?\\b({'|'.join(pos_verb)})\\b\"\n",
        "        neg_pattern3 = rf\"(?:{'|'.join(noun_phrases)})?.*?\\b({'|'.join(neg_adj)})(?:{'|'.join(adj_postfix)})?\\b.*?\\b({'|'.join(pos_verb)})\\b\"\n",
        "        neg_pattern4 = rf\"{'|'.join(noun_phrases)}.*({'|'.join(neg_adj)})(?:{verb_identifier1})?\"\n",
        "        neg_pattern5 = rf\"(?:{'|'.join(noun_phrases)})?{device_keywords}.*?\\b({'|'.join(pos_adj)})(?:{'|'.join(adj_postfix)})?\\b.*?\\b({'|'.join(neg_verb)})\\b\"\n",
        "        neg_pattern6 = rf\"(?:{'|'.join(noun_phrases)})?.*?\\b({'|'.join(pos_adj)})(?:{'|'.join(adj_postfix)})?\\b.*?\\b({'|'.join(neg_verb)})\\b\"\n",
        "\n",
        "\n",
        "        # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ø¯Ø± Ù…ØªÙ†\n",
        "        positive_matches1 = re.findall(pos_pattern1, text)\n",
        "        positive_matches2 = re.findall(pos_pattern2, text)\n",
        "        positive_matches3 = re.findall(pos_pattern3, text)\n",
        "        positive_matches4 = re.findall(pos_pattern4, text)\n",
        "        positive_matches5 = re.findall(pos_pattern5, text)\n",
        "        # print(positive_matches1, positive_matches2, positive_matches3, positive_matches4, positive_matches5)\n",
        "        # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ù†ÙÛŒ Ø¯Ø± Ù…ØªÙ†\n",
        "        negative_matches1 = re.findall(neg_pattern1, text)\n",
        "        negative_matches2 = re.findall(neg_pattern2, text)\n",
        "        negative_matches3 = re.findall(neg_pattern3, text)\n",
        "        negative_matches4 = re.findall(neg_pattern4, text)\n",
        "        negative_matches5 = re.findall(neg_pattern5, text)\n",
        "        negative_matches6 = re.findall(neg_pattern6, text)\n",
        "\n",
        "        # ØªØ¹ÛŒÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ·Ø§Ø¨Ù‚â€ŒÙ‡Ø§\n",
        "        if positive_matches1 or positive_matches2 or positive_matches3 or positive_matches4 or positive_matches5:\n",
        "            result = 'Ù…Ø«Ø¨Øª'\n",
        "        elif negative_matches1 or negative_matches2 or negative_matches3 or negative_matches4 or negative_matches5 or negative_matches6:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        else:\n",
        "          result = None\n",
        "\n",
        "        return result, text\n",
        "\n",
        "\n",
        "    def design(self, text):\n",
        "      # Define your regex pattern with conjunction words and dots\n",
        "      design_pattern = r\"(Ø²ÛŒØ¨Ø§|Ø®ÙˆØ´Ú¯Ù„|Ø²Ø´Øª|Ø´ÛŒÚ©|Ø¨Ø¯Ù†Ù‡|Ø¢Ù„ÙˆÙ…ÛŒÙ†ÛŒÙˆÙ…|Ø¸Ø§Ù‡Ø±)\"\n",
        "      conjunction_words = r\"\\s+(Ùˆ|Ø§Ù…Ø§|ÙˆÙ„ÛŒ|Ù„ÛŒÚ©Ù†)\\s+\"\n",
        "      split_pattern = f\"{conjunction_words}|[.ØŸ!]+\"\n",
        "\n",
        "      # Define lists of positive and negative words\n",
        "      positive_words = [\"Ø®ÙˆØ¨\", \"Ù‚Ø´Ù†Ú¯\", \"Ø¬Ø°Ø§Ø¨\", \"Ø´ÛŒÚ©\", \"Ø®ÙˆØ´Ú¯Ù„\", \"Ø²ÛŒØ¨Ø§\"]\n",
        "      negative_words = [\"Ø¶Ø¹ÛŒÙ\", \"Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±\", \"Ù…Ø´Ú©Ù„Ø§Øª\", \"Ù†Ø§Ø±Ø§Ø¶ÛŒ\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙ\" ,\"Ø¨Ø¯\", \"Ø¶Ø¹ÛŒÙ\", \"Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±\", \"Ù…Ø´Ú©Ù„Ø§Øª\", \"Ø¨ÛŒØ®ÙˆØ¯\", \"Ø¨ÛŒØ±ÛŒØ®Øª\"]\n",
        "\n",
        "      # Define a list of negative verbs\n",
        "      negative_verbs = [\"Ù†Ø¯Ø§Ø±Ø¯\", \"Ù†Ø¯Ø§Ø´Øª\", \"Ù†Ø¯Ø§Ø±Ù‡\", \"Ù†ÛŒØ³Øª\", \"Ù†ÛŒØ³ØªØ´\"]\n",
        "\n",
        "      result = None\n",
        "      if re.search(design_pattern, text):\n",
        "        positive = any(word in sentence for word in positive_words)\n",
        "        negative = any(word in sentence for word in negative_words)\n",
        "        has_negative_verb = any(verb in sentence for verb in negative_verbs)\n",
        "\n",
        "        if positive and not negative and not has_negative_verb:\n",
        "            result = 'Ù…Ø«Ø¨Øª'\n",
        "        elif not positive and negative:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        elif has_negative_verb:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        else:\n",
        "            result = 'Ø®Ù†Ø«ÛŒ'\n",
        "\n",
        "      return result, text\n",
        "\n",
        "\n",
        "    def camera(self, text):\n",
        "      # Define your regex pattern with conjunction words and dots\n",
        "      camera_pattern = r\"(Ø¹Ú©Ø³|ØªØµÙˆÛŒØ±|Ø±Ø²ÙˆÙ„ÙˆØ´Ù†|Ù…Ú¯Ø§Ù¾ÛŒÚ©Ø³Ù„|Ù…Ú¯Ø§ Ù¾ÛŒÚ©Ø³Ù„|Ù„Ù†Ø²|Ø´Ø§ØªØ±|ÙÙˆÚ©ÙˆØ³|Ù¾Ø±ØªØ±Ù‡|Ø¯ÛŒØ§ÙØ±Ø§Ú¯Ù…|Ø²ÙˆÙ…|Ù¾ÛŒÚ©Ø³Ù„|ÙÙ„Ø´|ÙÙ„Ø§Ø´|Ø§ÙˆÙ„ØªØ±Ø§ ÙˆØ§ÛŒØ¯| Ø§ÙˆÙ„ØªØ±Ø§ÙˆØ§ÛŒØ¯|Ù¾Ø±ÛŒØ³Ú©ÙˆÙ¾|ØªÙ„Ù‡ ÙÙˆØªÙˆ|ÙÙˆØªÙˆ|ÙØ±ÛŒÙ…|Ø¯ÙˆØ±Ø¨ÛŒÙ†|Ø¹Ú©Ø§Ø³ÛŒ|ÙÛŒÙ„Ù…|ÙØ±ÛŒÙ…|Ø³ÙˆÚ˜Ù‡|Ù„Ø±Ø²Ø´Ú¯ÛŒØ±|Ø³Ù„ÙÛŒ|ÙÛŒÙ„Ù…Ø¨Ø±Ø¯Ø§Ø±ÛŒ|ÙÛŒÙ„Ù…â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ|slow-motiom|slow motion|HDR|focus|resolution|pixel)\"\n",
        "      conjunction_words = r\"\\s+(Ùˆ|Ø§Ù…Ø§|ÙˆÙ„ÛŒ|Ù„ÛŒÚ©Ù†)\\s+\"\n",
        "      split_pattern = f\"{conjunction_words}|[.ØŸ!]+\"\n",
        "\n",
        "      # Define lists of positive and negative words\n",
        "      positive_words = [\"Ø®ÙÙ†\" ,\"ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡\" ,\"ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡\" ,\"Ø®ÙˆØ¨\", \"Ù‚Ø´Ù†Ú¯\", \"Ø¬Ø°Ø§Ø¨\", \"Ø´ÛŒÚ©\", \"Ø®ÙˆØ´Ú¯Ù„\", \"Ø²ÛŒØ¨Ø§\"]\n",
        "      negative_words = [\"Ù†Ø§Ø±Ø§Ø¶ÛŒ\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙ\" ,\"Ù…Ø²Ø®Ø±Ù\" ,\"Ú†Ø±Øª\" ,\"Ø¨Ø¯\", \"Ø¶Ø¹ÛŒÙ\", \"Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±\", \"Ù…Ø´Ú©Ù„Ø§Øª\", \"Ø¨ÛŒØ®ÙˆØ¯\", \"Ø¨ÛŒØ±ÛŒØ®Øª\"]\n",
        "\n",
        "      # Define a list of negative verbs\n",
        "      negative_verbs = [\"Ù†Ø¯Ø§Ø±Ø¯\", \"Ù†Ø¯Ø§Ø´Øª\", \"Ù†Ø¯Ø§Ø±Ù‡\", \"Ù†ÛŒØ³Øª\", \"Ù†ÛŒØ³ØªØ´\"]\n",
        "\n",
        "\n",
        "      result = None\n",
        "      if re.search(camera_pattern, text):\n",
        "        positive = any(word in sentence for word in positive_words)\n",
        "        negative = any(word in sentence for word in negative_words)\n",
        "        has_negative_verb = any(verb in sentence for verb in negative_verbs)\n",
        "\n",
        "        if positive and not negative and not has_negative_verb:\n",
        "            result = 'Ù…Ø«Ø¨Øª'\n",
        "        elif not positive and negative:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        elif has_negative_verb:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        else:\n",
        "            result = 'Ø®Ù†Ø«ÛŒ'\n",
        "\n",
        "      return result, text\n",
        "\n",
        "    def battery(self, text):\n",
        "      # Define your regex pattern with conjunction words and dots\n",
        "      battery_pattern = r\"(Ø¨Ø§ØªØ±ÛŒ|Ø¨Ø§Ø·Ø±ÛŒ|Ø´Ø§Ø±Ú˜|Ø´Ø§Ø±Ú˜Ø±|Ø¨Ø§ØªØ±ÛŒâ€ŒÙ‡Ø§|Ø¨Ø§ØªØ±ÛŒÙ‡Ø§)\"\n",
        "      conjunction_words = r\"\\s+(Ùˆ|Ø§Ù…Ø§|ÙˆÙ„ÛŒ|Ù„ÛŒÚ©Ù†)\\s+\"\n",
        "      split_pattern = f\"{conjunction_words}|[.ØŸ!]+\"\n",
        "\n",
        "      # Define lists of positive and negative words\n",
        "      positive_words = [\"Ø®ÙˆØ¨\", \"Ø¹Ø§Ù„ÛŒ\", \"Ø¶Ø¹ÛŒÙ\", \"Ù‚ÙˆÛŒ\", \"Ø¨Ø¯ÙˆÙ† Ù…Ø´Ú©Ù„\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨\"]\n",
        "      negative_words = [\"Ø¨Ø¯\", \"Ø¶Ø¹ÛŒÙ\", \"Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±\", \"Ù…Ø´Ú©Ù„Ø§Øª\", \"Ù†Ø§Ø±Ø§Ø¶ÛŒ\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙ\"]\n",
        "\n",
        "      # Define a list of negative verbs\n",
        "      negative_verbs = [\"Ù†Ø¯Ø§Ø±Ø¯\", \"Ù†Ø¯Ø§Ø´Øª\", \"Ù†Ø¯Ø§Ø±Ù‡\", \"Ù†ÛŒØ³Øª\", \"Ù†ÛŒØ³ØªØ´\"]\n",
        "\n",
        "      result = None\n",
        "      if re.search(battery_pattern, text):\n",
        "        positive = any(word in sentence for word in positive_words)\n",
        "        negative = any(word in sentence for word in negative_words)\n",
        "        has_negative_verb = any(verb in sentence for verb in negative_verbs)\n",
        "\n",
        "        if positive and not negative and not has_negative_verb:\n",
        "            result = 'Ù…Ø«Ø¨Øª'\n",
        "        elif not positive and negative:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        elif has_negative_verb:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        else:\n",
        "            result = 'Ø®Ù†Ø«ÛŒ'\n",
        "\n",
        "      return result, text\n",
        "\n",
        "    def processor(self, text):\n",
        "      # Define your regex pattern with conjunction words and dots\n",
        "      processor_pattern = r\"(Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±|GPU|Ø§Ù†ÙˆÛŒØ¯ÛŒØ§|chipset|Ú†ÛŒÙ¾Ø³Øª|core|Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡â€Œ|Ú†Ù†Ø¯ Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ|CPU|Ø§Ø³Ù†Ù¾ Ø¯Ø±Ø§Ú¯ÙˆÙ†|Ù…Ø¯ÛŒØ§ØªÚ©|MIPS|X64|X86|Ø§ÛŒÙ†ØªÙ„|ARM|Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ|ØªØ±Ø§Ø´Ù‡|Ø³ÛŒ Ù¾ÛŒ ÛŒÙˆ|Exynos|Ø§Ú¯Ø²ÛŒÙ†ÙˆØ³|Mediatek|Ø§Ø³Ù†Ù¾Ø¯Ø±Ø§Ú¯ÙˆÙ†|Ú©ÙˆØ§Ù„Ú©Ø§Ù…|intel|qualcomm|snapdragon)\"\n",
        "\n",
        "      # Define lists of positive and negative words\n",
        "      positive_words = [\"Ø®ÙˆØ¨\", \"Ø¹Ø§Ù„ÛŒ\", \"Ø¶Ø¹ÛŒÙ\", \"Ù‚ÙˆÛŒ\", \"Ø¨Ø¯ÙˆÙ† Ù…Ø´Ú©Ù„\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨\"]\n",
        "      negative_words = [\"Ø¨Ø¯\", \"Ø¶Ø¹ÛŒÙ\", \"Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±\", \"Ù…Ø´Ú©Ù„Ø§Øª\", \"Ù†Ø§Ø±Ø§Ø¶ÛŒ\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙ\"]\n",
        "\n",
        "      # Define a list of negative verbs\n",
        "      negative_verbs = [\"Ù†Ø¯Ø§Ø±Ø¯\", \"Ù†Ø¯Ø§Ø´Øª\", \"Ù†Ø¯Ø§Ø±Ù‡\", \"Ù†ÛŒØ³Øª\", \"Ù†ÛŒØ³ØªØ´\"]\n",
        "\n",
        "      result = None\n",
        "      if re.search(processor_pattern, text):\n",
        "        positive = any(word in sentence for word in positive_words)\n",
        "        negative = any(word in sentence for word in negative_words)\n",
        "        has_negative_verb = any(verb in sentence for verb in negative_verbs)\n",
        "\n",
        "        if positive and not negative and not has_negative_verb:\n",
        "            result = 'Ù…Ø«Ø¨Øª'\n",
        "        elif not positive and negative:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        elif has_negative_verb:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        else:\n",
        "            result = 'Ø®Ù†Ø«ÛŒ'\n",
        "\n",
        "      return result, text\n",
        "\n",
        "    def size(self, text):\n",
        "      # Define your regex pattern with conjunction words and dots\n",
        "      battery_pattern = r\"(Ø³Ù†Ú¯ÛŒÙ†|ÙˆØ²Ù†|Ú¯Ø±Ù…|Ø³Ø¨Ú©|Ø¬ÛŒØ¨|Ø§ÛŒÙ†Ú†|Ø¨Ø²Ø±Ú¯|Ú©ÙˆÚ†Ú©|Ú©ÙˆÚ†ÛŒÚ©|Ú¯Ù†Ø¯Ù‡|Ø®ÙˆØ´ Ø¯Ø³Øª)\"\n",
        "\n",
        "      # Define lists of positive and negative words\n",
        "      positive_words = [\"Ø®ÙˆØ¨\", \"Ø¹Ø§Ù„ÛŒ\", \"Ø¶Ø¹ÛŒÙ\", \"Ù‚ÙˆÛŒ\", \"Ø¨Ø¯ÙˆÙ† Ù…Ø´Ú©Ù„\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨\"]\n",
        "      negative_words = [\"Ø¨Ø¯\", \"Ø¶Ø¹ÛŒÙ\", \"Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±\", \"Ù…Ø´Ú©Ù„Ø§Øª\", \"Ù†Ø§Ø±Ø§Ø¶ÛŒ\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙ\"]\n",
        "\n",
        "      # Define a list of negative verbs\n",
        "      negative_verbs = [\"Ù†Ø¯Ø§Ø±Ø¯\", \"Ù†Ø¯Ø§Ø´Øª\", \"Ù†Ø¯Ø§Ø±Ù‡\", \"Ù†ÛŒØ³Øª\", \"Ù†ÛŒØ³ØªØ´\"]\n",
        "\n",
        "      result = None\n",
        "      if re.search(battery_pattern, text):\n",
        "        positive = any(word in sentence for word in positive_words)\n",
        "        negative = any(word in sentence for word in negative_words)\n",
        "        has_negative_verb = any(verb in sentence for verb in negative_verbs)\n",
        "\n",
        "        if positive and not negative and not has_negative_verb:\n",
        "            result = 'Ù…Ø«Ø¨Øª'\n",
        "        elif not positive and negative:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        elif has_negative_verb:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        else:\n",
        "            result = 'Ø®Ù†Ø«ÛŒ'\n",
        "\n",
        "      return result, text\n",
        "\n",
        "    def memory(self, text):\n",
        "      # Define your regex pattern with conjunction words and dots\n",
        "      memory_pattern = r\"(Ø±Ù…|Ø­Ø§ÙØ¸Ù‡|Ú¯ÛŒÚ¯|Ù…Ù…ÙˆØ±ÛŒ|Ø­Ø§ÙØ¸Ù‡ Ø¬Ø§Ù†Ø¨ÛŒ|Ø­Ø§ÙØ¸Ù‡ Ø¯Ø§Ø®Ù„ÛŒ|Ø¨Ø§ÛŒØª|Ú¯ÛŒÚ¯Ø§|Ù…Ú¯Ø§|ØªØ±Ø§|ÙØ¶Ø§|SD|Ù‡Ø§Ø±Ø¯|Ø¸Ø±ÙÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø³Ø§Ø²ÛŒ|Ø§Ø³ØªÙˆØ±ÛŒØ¬|storage)\"\n",
        "\n",
        "      # Define lists of positive and negative words\n",
        "      positive_words = [\"Ø®ÙˆØ¨\", \"Ø¹Ø§Ù„ÛŒ\", \"Ø¶Ø¹ÛŒÙ\", \"Ù‚ÙˆÛŒ\", \"Ø¨Ø¯ÙˆÙ† Ù…Ø´Ú©Ù„\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨\", 'Ø²ÛŒØ§Ø¯', 'Ø¨Ø§Ù„Ø§', 'Ø²ÛŒØ§Ø¯ÛŒ', 'Ø¨Ø§Ù„Ø§ÛŒÛŒ', 'Ú©Ø§ÙÛŒ']\n",
        "      negative_words = [\"Ø¨Ø¯\", \"Ø¶Ø¹ÛŒÙ\", \"Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±\", \"Ù…Ø´Ú©Ù„Ø§Øª\", \"Ù†Ø§Ø±Ø§Ø¶ÛŒ\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙ\", 'Ú©Ù…', 'Ù†Ø§Ú©Ø§ÙÛŒ', 'Ù…Ø­Ø¯ÙˆØ¯']\n",
        "\n",
        "      # Define a list of negative verbs\n",
        "      negative_verbs = [\"Ù†Ø¯Ø§Ø±Ø¯\", \"Ù†Ø¯Ø§Ø´Øª\", \"Ù†Ø¯Ø§Ø±Ù‡\", \"Ù†ÛŒØ³Øª\", \"Ù†ÛŒØ³ØªØ´\", ]\n",
        "\n",
        "      result = None\n",
        "      if re.search(memory_pattern, text):\n",
        "        positive = any(word in sentence for word in positive_words)\n",
        "        negative = any(word in sentence for word in negative_words)\n",
        "        has_negative_verb = any(verb in sentence for verb in negative_verbs)\n",
        "\n",
        "        if positive and not negative and not has_negative_verb:\n",
        "            result = 'Ù…Ø«Ø¨Øª'\n",
        "        elif not positive and negative:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        elif has_negative_verb:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        else:\n",
        "            result = 'Ø®Ù†Ø«ÛŒ'\n",
        "\n",
        "      return result, text\n",
        "\n",
        "    def screen(self, text):\n",
        "      # Define your regex pattern with conjunction words and dots\n",
        "\n",
        "      # TODO --------------------\n",
        "\n",
        "\n",
        "      screen_pattern = r\"(Ø±Ù…|Ø­Ø§ÙØ¸Ù‡|Ú¯ÛŒÚ¯|Ù…Ù…ÙˆØ±ÛŒ|Ø­Ø§ÙØ¸Ù‡ Ø¬Ø§Ù†Ø¨ÛŒ|Ø­Ø§ÙØ¸Ù‡ Ø¯Ø§Ø®Ù„ÛŒ|Ø¨Ø§ÛŒØª|Ú¯ÛŒÚ¯Ø§|Ù…Ú¯Ø§|ØªØ±Ø§|ÙØ¶Ø§|SD|Ù‡Ø§Ø±Ø¯|Ø¸Ø±ÙÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø³Ø§Ø²ÛŒ|Ø§Ø³ØªÙˆØ±ÛŒØ¬|storage)\"\n",
        "\n",
        "      # TODO -----------------------\n",
        "\n",
        "      # Define lists of positive and negative words\n",
        "      positive_words = [\"Ø®ÙˆØ¨\", \"Ø¹Ø§Ù„ÛŒ\", \"Ø¶Ø¹ÛŒÙ\", \"Ù‚ÙˆÛŒ\", \"Ø¨Ø¯ÙˆÙ† Ù…Ø´Ú©Ù„\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨\", 'Ø²ÛŒØ§Ø¯', 'Ø¨Ø§Ù„Ø§', 'Ø²ÛŒØ§Ø¯ÛŒ', 'Ø¨Ø§Ù„Ø§ÛŒÛŒ', 'Ú©Ø§ÙÛŒ']\n",
        "      negative_words = [\"Ø¨Ø¯\", \"Ø¶Ø¹ÛŒÙ\", \"Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±\", \"Ù…Ø´Ú©Ù„Ø§Øª\", \"Ù†Ø§Ø±Ø§Ø¶ÛŒ\", \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙ\", 'Ú©Ù…', 'Ù†Ø§Ú©Ø§ÙÛŒ', 'Ù…Ø­Ø¯ÙˆØ¯']\n",
        "\n",
        "      # Define a list of negative verbs\n",
        "      negative_verbs = [\"Ù†Ø¯Ø§Ø±Ø¯\", \"Ù†Ø¯Ø§Ø´Øª\", \"Ù†Ø¯Ø§Ø±Ù‡\", \"Ù†ÛŒØ³Øª\", \"Ù†ÛŒØ³ØªØ´\", ]\n",
        "\n",
        "      result = None\n",
        "      if re.search(screen_pattern, text):\n",
        "        positive = any(word in sentence for word in positive_words)\n",
        "        negative = any(word in sentence for word in negative_words)\n",
        "        has_negative_verb = any(verb in sentence for verb in negative_verbs)\n",
        "\n",
        "        if positive and not negative and not has_negative_verb:\n",
        "            result = 'Ù…Ø«Ø¨Øª'\n",
        "        elif not positive and negative:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        elif has_negative_verb:\n",
        "            result = 'Ù…Ù†ÙÛŒ'\n",
        "        else:\n",
        "            result = 'Ø®Ù†Ø«ÛŒ'\n",
        "\n",
        "      return result, text\n",
        "\n",
        "\n",
        "    def run(self, text):\n",
        "        sentences = persian_sentence_tokenizer_v3(text)\n",
        "        result = {}\n",
        "\n",
        "        result[\"Ù†Ø¸Ø± Ú©Ù„ÛŒ\"] = []\n",
        "        for sent in sentences:\n",
        "          general_opinion_res = self.general_opinion(sent)\n",
        "          if general_opinion_res[0]:\n",
        "              result[\"Ù†Ø¸Ø± Ú©Ù„ÛŒ\"].append({\n",
        "                  \"result\": general_opinion_res[0]#TODO\n",
        "                  , \"sentence\": general_opinion_res[1]#TODO\n",
        "                  })\n",
        "\n",
        "        result[\"Ø·Ø±Ø§Ø­ÛŒ\"] = []\n",
        "        for sent in sentences:\n",
        "          design_res = self.design(sent)\n",
        "          if design_res[0]:\n",
        "              result[\"Ø·Ø±Ø§Ø­ÛŒ\"].append({\n",
        "                  \"result\": design_res[0]#TODO\n",
        "                  , \"sentence\": design_res[1]#TODO\n",
        "                  })\n",
        "\n",
        "        result[\"Ø¯ÙˆØ±Ø¨ÛŒÙ†\"] = []\n",
        "        for sent in sentences:\n",
        "          camera_res = self.camera(sent)\n",
        "          if camera_res[0]:\n",
        "              result[\"Ø¯ÙˆØ±Ø¨ÛŒÙ†\"].append({\n",
        "                  \"result\": camera_res[0]#TODO\n",
        "                  , \"sentence\": camera_res[1]#TODO\n",
        "                  })\n",
        "\n",
        "        result[\"Ø¨Ø§Ø·Ø±ÛŒ\"] = []\n",
        "        for sent in sentences:\n",
        "          battery_res = self.battery(sent)\n",
        "          if battery_res[0]:\n",
        "              result[\"Ø¨Ø§Ø·Ø±ÛŒ\"].append({\n",
        "                  \"result\": battery_res[0]#TODO\n",
        "                  , \"sentence\": battery_res[1]#TODO\n",
        "                  })\n",
        "\n",
        "        result[\"Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡\"] = []\n",
        "        for sent in sentences:\n",
        "          processor_res = self.processor(sent)\n",
        "          if processor_res[0]:\n",
        "              result[\"Ø¨Ø§Ø·Ø±ÛŒ\"].append({\n",
        "                  \"result\": processor_res[0]#TODO\n",
        "                  , \"sentence\": processor_res[1]#TODO\n",
        "                  })\n",
        "\n",
        "        result[\"Ø³Ø§ÛŒØ²\"] = []\n",
        "        for sent in sentences:\n",
        "          size_res = self.size(sent)\n",
        "          if size_res[0]:\n",
        "              result[\"Ø¨Ø§Ø·Ø±ÛŒ\"].append({\n",
        "                  \"result\": size_res[0]#TODO\n",
        "                  , \"sentence\": size_res[1]#TODO\n",
        "                  })\n",
        "\n",
        "        result[\"Ø­Ø§ÙØ¸Ù‡\"] = []\n",
        "        for sent in sentences:\n",
        "          memory_res = self.memory(sent)\n",
        "          if memory_res[0]:\n",
        "              result[\"Ø­Ø§ÙØ¸Ù‡\"].append({\n",
        "                  \"result\": memory_res[0]#TODO\n",
        "                  , \"sentence\": memory_res[1]#TODO\n",
        "                  })\n",
        "\n",
        "        result[\"ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´\"] = []\n",
        "        for sent in sentences:\n",
        "          screen_res = self.screen(sent)\n",
        "          if screen_res[0]:\n",
        "              result[\"ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´\"].append({\n",
        "                  \"result\": screen_res[0]#TODO\n",
        "                  , \"sentence\": screen_res[1]#TODO\n",
        "                  })\n",
        "\n",
        "        # feats = list(result.keys())\n",
        "        # cols = ['Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ', 'Ø¬Ù…Ù„Ù‡']\n",
        "        # print(feats)\n",
        "        # print(result[feats[0]])\n",
        "        # data_frame = pd.DataFrame(\n",
        "        #                           np.array([[result[feat]['result'], result[feat]['sentence']] for feat in feats]),\n",
        "        #                           columns=cols)\n",
        "\n",
        "\n",
        "        return result\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yG6z7_C78GgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "OoFaTaezQ7-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize model and run**"
      ],
      "metadata": {
        "id": "UAtqNbTcZel1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SmartPhoneFeatureExtractor()"
      ],
      "metadata": {
        "id": "zLI6z03BLWsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments = [\n",
        "    'Ú¯ÙˆØ´ÛŒ ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ù†ÛŒØ³Øª',\n",
        "    'Ø®ÙˆØ¨ÛŒ Ù†ÛŒØ³Øª',\n",
        "    'Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø¨Ø¯ÛŒ Ø§Ø³Øª',\n",
        "    'Ú¯ÙˆØ´ÛŒ Ø®ÙˆØ¨ÛŒ Ù‡Ø³Øª',\n",
        "    'Ø¯Ø± Ú©Ù„ Ø§Ø² Ú¯ÙˆØ´ÛŒ Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù†Ø¯ Ø§ÛŒØ´',\n",
        "    'Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨ Ø¨ÙˆØ¯',\n",
        "    'Ù‡Ù…Ù‡ Ú†ÛŒØ²Ø´ Ø®ÙˆØ¨ Ø¨ÙˆØ¯',\n",
        "    'Ø§ÛŒÙ† Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯',\n",
        "    'Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ù†Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡ Ùˆ ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª 50 Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡ Ùˆ Ø±Ø§Ø¶ÛŒØªÙˆÙ† Ù…ÛŒÚ©Ù†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡ Ø§Ø´ Ø§ØµÙ„Ø§ Ù‡Ù†Ú¯ Ù†Ù…ÛŒÚ©Ù†Ù‡ Ùˆ Ø¯Ø§Øº Ù†Ù…ÛŒØ´Ù‡ Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡ ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´Ø´ Ø¨Ø§ 900 Ù†ÛŒØª Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„Ø§ ØªØµØ§ÙˆÛŒØ± Ø±Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ùˆ ÙˆØ§Ø¶Ø¹ Ù†Ø´ÙˆÙ† Ù…ÛŒØ¯Ù‡. Ø¸Ø§Ù‡Ø± Ú©ÙˆØ´ÛŒ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø´ÛŒÚ© Ùˆ Ø®ÙˆØ´Ú¯Ù„Ù‡. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø­Ø°Ø§Ø¨ÛŒ Ø¯Ø§Ø±Ù‡. Ø¯Ø± Ú©Ù„ Ú¯ÙˆØ´ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ Ø®ÛŒÙ„ÛŒ Ø§Ø±Ø²ÙˆÙ†Ù‡. Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨Ù‡.'\n",
        "]"
      ],
      "metadata": {
        "id": "yFjG6z8vISWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_comemnt(model, comment):\n",
        "  normalized_text = model.sent_normalize(comment)\n",
        "  print('input: ', normalized_text)\n",
        "  res = model.run(normalized_text)\n",
        "  print('output: ', )\n",
        "  for feat in res.keys():\n",
        "    print(f'{feat}: {res[feat]}')"
      ],
      "metadata": {
        "id": "9gKgPBDzObD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = persian_sentence_tokenizer_v3(comments[-1])\n",
        "sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijPGjyzoUcTJ",
        "outputId": "3aa91b62-8bcc-4739-e70a-a62565e43d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ù†Ø¯Ù‡ Ø§Ø³Øª.',\n",
              " 'Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡',\n",
              " 'Ùˆ ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª 50 Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡',\n",
              " 'Ùˆ Ø±Ø§Ø¶ÛŒØªÙˆÙ† Ù…ÛŒÚ©Ù†Ù‡',\n",
              " 'Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡ Ø§Ø´ Ø§ØµÙ„Ø§ Ù‡Ù†Ú¯ Ù†Ù…ÛŒÚ©Ù†Ù‡ Ùˆ Ùˆ Ø¯Ø§Øº Ù†Ù…ÛŒØ´Ù‡',\n",
              " 'Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡',\n",
              " 'ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´Ø´ Ø¨Ø§ 900 Ù†ÛŒØª Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„Ø§ ØªØµØ§ÙˆÛŒØ± Ø±Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ùˆ ÙˆØ§Ø¶Ø¹ Ù†Ø´ÙˆÙ† Ù…ÛŒØ¯Ù‡ Ùˆ .',\n",
              " 'Ø¸Ø§Ù‡Ø± Ú©ÙˆØ´ÛŒ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø´ÛŒÚ© Ùˆ Ø®ÙˆØ´Ú¯Ù„Ù‡.',\n",
              " 'Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø­Ø°Ø§Ø¨ÛŒ Ø¯Ø§Ø±Ù‡.',\n",
              " 'Ø¯Ø± Ú©Ù„ Ú¯ÙˆØ´ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ Ø®ÛŒÙ„ÛŒ Ø§Ø±Ø²ÙˆÙ†Ù‡.',\n",
              " 'Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨Ù‡.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_comemnt(model, comments[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgbwl7IdO-rz",
        "outputId": "7942fcc4-d810-4c02-8463-afa428fc8554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡ Ùˆ ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª ÛµÛ° Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡ Ùˆ Ø±Ø§Ø¶ÛŒØªÙˆÙ† Ù…ÛŒÚ©Ù†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡ Ø§Ø´ Ø§ØµÙ„Ø§ Ù‡Ù†Ú¯ Ù†Ù…ÛŒÚ©Ù†Ù‡ Ùˆ Ø¯Ø§Øº Ù†Ù…ÛŒØ´Ù‡ Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡ ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´Ø´ Ø¨Ø§ Û¹Û°Û° Ù†ÛŒØª Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„Ø§ ØªØµØ§ÙˆÛŒØ± Ø±Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ùˆ ÙˆØ§Ø¶Ø¹ Ù†Ø´ÙˆÙ† Ù…ÛŒØ¯Ù‡. Ø¸Ø§Ù‡Ø± Ú©ÙˆØ´ÛŒ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø´ÛŒÚ© Ùˆ Ø®ÙˆØ´Ú¯Ù„Ù‡. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø­Ø°Ø§Ø¨ÛŒ Ø¯Ø§Ø±Ù‡. Ø¯Ø± Ú©Ù„ Ú¯ÙˆØ´ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ Ø®ÛŒÙ„ÛŒ Ø§Ø±Ø²ÙˆÙ†Ù‡. Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨Ù‡.\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ø¯Ù‡ Ùˆ Ø§Ø³Øª Ùˆ .'}, {'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ø¯Ø± Ú©Ù„ Ú¯ÙˆØ´ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ Ø®ÛŒÙ„ÛŒ Ø§Ø±Ø²ÙˆÙ†Ù‡.'}, {'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨Ù‡.'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: [{'result': 'Ø®Ù†Ø«ÛŒ', 'sentence': 'Ø¸Ø§Ù‡Ø± Ú©ÙˆØ´ÛŒ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø´ÛŒÚ© Ùˆ Ø®ÙˆØ´Ú¯Ù„Ù‡.'}]\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: [{'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ùˆ ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª ÛµÛ° Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡'}]\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: [{'result': 'Ø®Ù†Ø«ÛŒ', 'sentence': 'Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡'}, {'result': 'Ø®Ù†Ø«ÛŒ', 'sentence': 'Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡'}]\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for com in comments:\n",
        "  process_comemnt(model, com)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iECkseoV5v3l",
        "outputId": "08bd3570-edaa-4010-d623-b95fcd915a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  Ú¯ÙˆØ´ÛŒ ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ù†ÛŒØ³Øª\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ù†ÙÛŒ', 'sentence': 'Ú¯ÙˆØ´ÛŒ ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ù†ÛŒØ³Øª'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: []\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: []\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: []\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n",
            "input:  Ø®ÙˆØ¨ÛŒ Ù†ÛŒØ³Øª\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ù†ÙÛŒ', 'sentence': 'Ø®ÙˆØ¨ÛŒ Ù†ÛŒØ³Øª'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: []\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: []\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: []\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n",
            "input:  Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø¨Ø¯ÛŒ Ø§Ø³Øª\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ù†ÙÛŒ', 'sentence': 'Ù…ÙˆØ¨Ø§ÛŒÙ„ Ø¨Ø¯ÛŒ Ø§Ø³Øª'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: []\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: []\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: []\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n",
            "input:  Ú¯ÙˆØ´ÛŒ Ø®ÙˆØ¨ÛŒ Ù‡Ø³Øª\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ú¯ÙˆØ´ÛŒ Ø®ÙˆØ¨ÛŒ Ù‡Ø³Øª'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: []\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: []\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: []\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n",
            "input:  Ø¯Ø± Ú©Ù„ Ø§Ø² Ú¯ÙˆØ´ÛŒ Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù†Ø¯ Ø§ÛŒØ´\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ø¯Ø± Ú©Ù„ Ø§Ø² Ú¯ÙˆØ´ÛŒ Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù†Ø¯ Ø§ÛŒØ´'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: []\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: []\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: []\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n",
            "input:  Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨ Ø¨ÙˆØ¯\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨ Ø¨ÙˆØ¯'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: []\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: []\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: []\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n",
            "input:  Ù‡Ù…Ù‡ Ú†ÛŒØ²Ø´ Ø®ÙˆØ¨ Ø¨ÙˆØ¯\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ù‡Ù…Ù‡ Ú†ÛŒØ²Ø´ Ø®ÙˆØ¨ Ø¨ÙˆØ¯'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: []\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: []\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: []\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n",
            "input:  Ø§ÛŒÙ† Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ù†ÙÛŒ', 'sentence': 'Ø§ÛŒÙ† Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: []\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: []\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: []\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n",
            "input:  Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡ Ùˆ ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª ÛµÛ° Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡ Ùˆ Ø±Ø§Ø¶ÛŒØªÙˆÙ† Ù…ÛŒÚ©Ù†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡ Ø§Ø´ Ø§ØµÙ„Ø§ Ù‡Ù†Ú¯ Ù†Ù…ÛŒÚ©Ù†Ù‡ Ùˆ Ø¯Ø§Øº Ù†Ù…ÛŒØ´Ù‡ Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡ ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´Ø´ Ø¨Ø§ Û¹Û°Û° Ù†ÛŒØª Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„Ø§ ØªØµØ§ÙˆÛŒØ± Ø±Ùˆ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ùˆ ÙˆØ§Ø¶Ø¹ Ù†Ø´ÙˆÙ† Ù…ÛŒØ¯Ù‡. Ø¸Ø§Ù‡Ø± Ú©ÙˆØ´ÛŒ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø´ÛŒÚ© Ùˆ Ø®ÙˆØ´Ú¯Ù„Ù‡. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ø­Ø°Ø§Ø¨ÛŒ Ø¯Ø§Ø±Ù‡. Ø¯Ø± Ú©Ù„ Ú¯ÙˆØ´ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ Ø®ÛŒÙ„ÛŒ Ø§Ø±Ø²ÙˆÙ†Ù‡. Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨Ù‡.\n",
            "output: \n",
            "Ù†Ø¸Ø± Ú©Ù„ÛŒ: [{'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ú¯ÙˆØ´ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ú©Ù†Ø¯Ù‡ Ùˆ Ø§Ø³Øª Ùˆ .'}, {'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ø¯Ø± Ú©Ù„ Ú¯ÙˆØ´ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‚ÛŒÙ…ØªØ´ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ Ø®ÛŒÙ„ÛŒ Ø§Ø±Ø²ÙˆÙ†Ù‡.'}, {'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ø¯Ø± Ú©Ù„ Ø®ÙˆØ¨Ù‡.'}]\n",
            "Ø·Ø±Ø§Ø­ÛŒ: [{'result': 'Ø®Ù†Ø«ÛŒ', 'sentence': 'Ø¸Ø§Ù‡Ø± Ú©ÙˆØ´ÛŒ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø´ÛŒÚ© Ùˆ Ø®ÙˆØ´Ú¯Ù„Ù‡.'}]\n",
            "Ø¯ÙˆØ±Ø¨ÛŒÙ†: [{'result': 'Ù…Ø«Ø¨Øª', 'sentence': 'Ùˆ ØªÙˆ Ù†ÛŒÙ… Ø³Ø§Ø¹Øª ÛµÛ° Ø¯Ø±ØµØ¯Ø´ Ù¾Ø± Ù…ÛŒØ´Ù‡ Ø¯ÙˆØ±Ø¨ÛŒÙ† ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ù‡'}]\n",
            "Ø¨Ø§Ø·Ø±ÛŒ: [{'result': 'Ø®Ù†Ø«ÛŒ', 'sentence': 'Ø¨Ø§Ø·Ø±ÛŒ Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø±ÙˆØ² Ù¾Ø± Ú©Ø§Ø± Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ù…ÛŒÚ©Ù†Ù‡'}, {'result': 'Ø®Ù†Ø«ÛŒ', 'sentence': 'Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±Ùˆ Ø±Ø§Ø­Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒÚ©Ù†Ù‡'}]\n",
            "Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡: []\n",
            "Ø³Ø§ÛŒØ²: []\n",
            "Ø­Ø§ÙØ¸Ù‡: []\n",
            "ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´: []\n"
          ]
        }
      ]
    }
  ]
}